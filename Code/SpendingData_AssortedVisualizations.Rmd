---
title: ""
output:
  html_document:
    df_print: paged
---

```{r, echo = FALSE}
knitr:: opts_chunk$set(eval = FALSE)
```

### Reading in dataframes

```{r}
# Data manipulation
library(data.table); library(dtplyr); library(tidyverse);

# Consumer Price Index inflation corrections
library(quantmod)

# Generating Sankey diagrams
library(networkD3); library(magrittr); library(htmlTable); library(htmltools); library(htmlwidgets)

# When ICE, CBP, and EOIR are awarding agencies:
alldat = fread(file = "../data/filtered_data_awarding.csv", sep = ",", header = TRUE)
cat("Awarding Transactions:\n")
nrow(alldat) # 289340 awarding transactions

# When ICE, CBP, and EOIR are funding agencies:
fund = fread(file = "../data/filtered_data_funding.csv", sep = ",", header = TRUE)
cat("Funding Transactions:\n")
nrow(fund) # 154261 funding transactions

# NAICS code table:
naics_codes = fread(file = "../data/naics_codes.csv", sep = ",", header = TRUE)
naics_codes = naics_codes %>% select(c(1:3))
colnames(naics_codes) = c("seq", "naics", "naics_title")

# Immigration statistics:
apprehensions = read.csv(file = "../data/app_by_country.csv", header = TRUE)
app_cleaned = apprehensions %>% 
  select(-X) %>% 
  gather(key = "year", value = "frequency", -Region.and.country.of.nationality) %>%
  rename("origin" = Region.and.country.of.nationality) %>%
  mutate(year = as.numeric(gsub("X", "", year)),
         frequency = as.numeric(frequency)) %>%
  na.exclude(frequency)
```

### How to best account for duplicates in the data?

In past class exercises, we defined unique transactions in our "transaction.csv" file as those row entries which had differences in either award_id, action_date, or total_obligation. After some exploration, I think that the "universal_transaction_matview" 

```{r}
# Binding all rows together and taking distinct values
alldat = bind_rows(alldat, fund) %>% 
  distinct(award_id, action_date, total_obligation, generated_pragmatic_obligation, transaction_description, .keep_all = TRUE)
  # distinct(award_id, .keep_all = TRUE)

cat("Total Transactions:\n")
nrow(alldat) # 300319 (Few times where these agencies funded, but did not award)
```

### Duplicated transaction IDs

To make sure that we don't have a ton of duplicated transactions in the dataset, I checked the number of observations occuring for each transaction ID. The resulting table shows that some transaction IDs are repeated over 100 times each in the dataset, which may be problematic:

```{r}
alldat %>% group_by(award_id) %>% summarise(count = n()) %>% arrange(desc(count)) %>% head()
```

But on further inspectioon, these appear to renewed contracts with the same ID. Below are the first 6 rows for the most common transaction ID, which looks like a contract for guard service that is continued every year. An important note here is that the total obligation is consistent across all of these shared transactions, but the pragmatic obligation varies substantially. 

```{r}
alldat[alldat$award_id == 69245855,]
```

To me, this seems like evidence we should be working with the pragmatic obligation, not just the total obligation.

### Distribution of pragmatic obligation

As additional error checking, I've also plotted the distribution of generated obligations, both raw and the log10 + 1 transformation of absolute values. Most transactions seem to have zero cost or are relatively small -- in the range of 100 - 10,000 dollars -- but there are a few extremes. 

```{r}
# Getting consumer price index values
getSymbols("CPIAUCSL", src='FRED') # Use Federal Reserve Data
avg.cpi <- apply.yearly(CPIAUCSL, mean)
cpi.correction <- avg.cpi/as.numeric(avg.cpi['2019'])

cpi.correction = data.frame(cpi.correction) %>% 
  rownames_to_column() %>% 
  rename("date" = "rowname", "correction" = "CPIAUCSL") %>%
  mutate(year = year(as.Date(date))) %>%
  select(-date)

alldat = left_join(alldat, cpi.correction, by = c("fiscal_year" = "year"), copy = TRUE) %>% 
  mutate(generated_pragmatic_obligation = generated_pragmatic_obligation / correction,
         total_obligation = total_obligation / correction)

alldat %>% 
  ggplot(aes(x = generated_pragmatic_obligation)) +
  geom_density(fill = "blue", alpha = .25) +
  ggtitle("Obligation Frequency")

alldat %>% 
  ggplot(aes(x = log10(abs(generated_pragmatic_obligation) + 1))) +
  geom_density(fill = "blue", alpha = .25) +
  ggtitle("Obligation Frequency (Log10(x + 1) Transformed)") 
```

As an example, I've highlighted the most expensive single transaction below. This looks like some massive overspending for some sort of fencing project:

```{r}
alldat[alldat$generated_pragmatic_obligation == max(alldat$generated_pragmatic_obligation),]
```

### Does total yearly obligation match government figures?

Below is a table of official statistics reported by the current presidential administration on border funding:

![](../Figures/Official_Budget.png)

When we compare these with spending totals generated from our data, it seems that the USASpending dataset is likely under-reporting by 50% or more. Aside from an error in my code, this could be the result of limited reporting requirements of the Department of Homeland Security to this dataset. 

```{r, echo = FALSE}
# Total Obligation
total_plot = alldat %>% 
  group_by(fiscal_year) %>%
  summarise(total_obligation = sum(total_obligation)) %>% 
  mutate(total_obligation_Bn = total_obligation / 1e9) %>%
  ggplot(aes(x = fiscal_year,
             y = total_obligation_Bn)) +
  geom_line(color = "red") +
  ggtitle("Total Obligation over Time") + 
  xlab("Fiscal Year") +
  ylab("Total Obligation (in Billions)")

# Pragmatic Obligation
prag_plot = alldat %>% 
  group_by(fiscal_year) %>% 
  summarise(total_prag_obligation = sum(generated_pragmatic_obligation)) %>% 
  mutate(total_prag_obligation_Bn = total_prag_obligation / 1e9) %>%
  ggplot(aes(x = fiscal_year,
             y = total_prag_obligation_Bn)) +
  geom_line(color = "forestgreen") +
  ggtitle("Total Pragmatic Obligation over Time") + 
  xlab("Fiscal Year") +
  ylab("Total Pragmatic Obligation (in Billions)")


# Non-negative pragmatic obligation
nonneg_prag_plot = alldat %>% 
  filter(generated_pragmatic_obligation > 0) %>%
  group_by(fiscal_year) %>%
  summarise(nonneg_prag_obligation = sum(generated_pragmatic_obligation)) %>% 
  mutate(nonneg_prag_obligation_Bn = nonneg_prag_obligation / 1e9) %>%
  ggplot(aes(x = fiscal_year,
             y = nonneg_prag_obligation_Bn)) +
  geom_line(color = "blue") +
  ggtitle("Total Nonneg Pragmatic Obligation over Time") + 
  xlab("Fiscal Year") +
  ylab("Nonnegative Pragmatic Obligation (in Billions)")

gridExtra::grid.arrange(total_plot, prag_plot, nonneg_prag_plot, nrow = 1)

```

### Total obligation over time

```{r}
# Creating new data column for group by computation
# 1/1/1970 appears to be the correct start date
alldat$action_date = as.Date(alldat$action_date, origin = "1970-01-01")

# Plotting total spending over time
alldat %>% 
  mutate(month = month(action_date), 
         year  = year(action_date)) %>%
  group_by(month, year) %>%
  
  # Calculating sum of generated pragmatic obligation
  summarise(total_spending = sum(generated_pragmatic_obligation)) %>%
  
  # Creating new date column for plotting
  mutate(plotdate = as.Date(paste("1-", month,"-", year, sep = ""), format = "%d-%m-%Y")) %>%
  
  # Plotting figure
  ggplot(aes(x = plotdate,
             y = total_spending / 1e6)) +
  geom_line() +
  ggtitle("Total Obligation Over Time in 2018 Dollars") +
  xlab("Date") +
  ylab("Total Pragmatic Obligation (Millions $)")
```


### Exploring Spending Patterns with NAICS codes

NAICS codes are present for a large number of transactions within this dataset and may be useful in detecting patterns of spending priorities for the agencies in question. Conveniently, they offer a nested method to classifying transactions that correspond to:

* Sector: 2-digit code
* Subsector: 3-digit code
* Industry Group: 4-digit code
* NAICS Industry: 5-digit code
* National Industry: 6-digit code

```{r}
# Removes all entries without NAICS codes
alldat = alldat %>% drop_na(naics_code)

# Creating substrings of 2, 3, 4, 5, and 6 digits
alldat = alldat %>%
  mutate(naics_d2 = substr(alldat$naics_code, 0, 2),
                  naics_d3 = substr(alldat$naics_code, 0, 3),
                  naics_d4 = substr(alldat$naics_code, 0, 4),
                  naics_d5 = substr(alldat$naics_code, 0, 5),
                  naics_d6 = substr(alldat$naics_code, 0, 6))

# To add NAICS names for each substring,  
for(digit in c(2:6)){

  alldat = alldat %>% 
    left_join(naics_codes %>% select(naics, naics_title), by = setNames( "naics", sprintf("naics_d%s", digit)))
  
  colnames(alldat)[colnames(alldat) == "naics_title"] = sprintf("title_d%s", digit)
}

write.csv(alldat, "../Data/transaction_data_NAICS.csv")
```

### What are the most common types of spending at each hierarchichal level of NAICS organization?

To show variation in spending types across our hierarchical NAICS codes, I've chosen to use a Sankey Diagram, which can show "flow" of a variable through a series of edges and nodes. 

Code adapted from https://towardsdatascience.com/using-networkd3-in-r-to-create-simple-and-clear-sankey-diagrams-48f8ba8a4ace

```{r}
# Filter to just the last 10 fiscal years
sankey_data = alldat %>% filter(fiscal_year > 2008)

# Filter the dataset to interested subtier agencies
edges = sankey_data %>% filter(awarding_subtier_agency_name %in% 
                    c("U.S. Customs and Border Protection", "U.S. Immigration and Customs Enforcement")) %>%
  filter(title_d2 != "NA") %>% # Where a joined NAICS title is present
  group_by(awarding_subtier_agency_name, title_d2) %>% # Group by two-digit titles 
  summarise(value = sum(generated_pragmatic_obligation)) %>% # Sum obligation
  mutate(rank = rank(desc(value))) %>%
  arrange(desc(value)) %>%
  
  # To increase readability of the figure, I've only chosen to show the three most common NAICS sector codes
  mutate(title_d2 = case_when(rank < 4 ~ title_d2,
                              rank >= 4 ~ "Other")) %>%
  
  # Regroup following rename operation and retabulate spending
  group_by(awarding_subtier_agency_name, title_d2) %>%  
  summarise(value = sum(value)) %>%
  ungroup()

# Repeat the same process as above for 3-digit codes
edges_2 = sankey_data %>% filter(title_d2 %in% unique(edges$title_d2)) %>%
  filter(title_d2 != "NA") %>%
  group_by(title_d2, title_d3) %>%
  summarise(value = sum(generated_pragmatic_obligation)) %>%
  mutate(rank = rank(desc(value))) %>%
  arrange(desc(value)) %>%
  group_by(title_d2, title_d3) %>%  
  summarise(value = sum(value)) %>%
  ungroup()

# Repeat the same process as above for 3-digit codes
edges_3 = sankey_data %>% filter(title_d3 %in% unique(edges_2$title_d3)) %>%
  filter(title_d3 != "NA") %>%
  group_by(title_d3, title_d4) %>%
  summarise(value = sum(generated_pragmatic_obligation)) %>%
  mutate(rank = rank(desc(value))) %>%
  arrange(desc(value)) %>%
  group_by(title_d3, title_d4) %>%  
  summarise(value = sum(value)) %>%
  ungroup()

# Rename columns between all files to prepare for joining
colnames(edges) = c("source", "target", "value")
colnames(edges_2) = c("source", "target", "value")
colnames(edges_3) = c("source", "target", "value")

# Join together and remove instances where a sources and targets are shared (lacking additional specificity in NAICS table)
edges = bind_rows(edges, edges_2, edges_3)
edges = edges %>% filter(source != target)

# Creating table of distinct sources
sources <- edges %>%
  distinct(source) %>%
  rename(label = source)

# Creating table of distinct targets
targets <- edges %>%
  distinct(target) %>%
  rename(label = target)

# Join these all together to generate a table of names and indices (nodes)
nodes <- full_join(sources, targets, by = "label")
nodes <- nodes %>% rowid_to_column("id") %>% mutate(id = id - 1)

# Create list of connections (edges)
# Attaching IDs to where a node is the soruce
edges <- edges %>% 
  left_join(nodes, by = c("source" = "label")) %>% 
  rename(from = id)

# Attaching IDs to where a node is the target
edges <- edges %>% 
  left_join(nodes, by = c("target" = "label")) %>% 
  rename(to = id)

# Remove name vector
edges <- select(edges, from, to, value)

# Rescale
edges$value = edges$value / 1e6

# Generate Sankey Network Diagram
sankeyNetwork(edges, nodes, Source = "from", Target = "to", Value = "value",
              NodeID = "label", units = "Million USD", fontSize = 12, nodeWidth = 30, sinksRight = FALSE,
              fontFamily = "sans-serif", height = 750, width = 1500) %>%
  # Can add title here, if desired
  # htmlwidgets::prependContent(htmltools::tags$h1("C.B.P. and I.C.E. Spending Patterns FY2009 - FY2019")) %>% 
  saveNetwork(file = 'spending_sankey.html')
```

### How do frequencies of these funding categories vary over time?

Checking to make sure that I'm being relatively consistent in my classification labels (no strange sub-levels)

```{r}
naics_codes %>% filter(grep("Computer", naics_title))
naics_codes %>% filter(grep("5415", naics))

naics_codes %>% filter(grep("Security", naics_title))
naics_codes %>% filter(grep("5616", naics))

naics_codes %>% filter(grep("Information", naics_title))
naics_codes %>% filter(grep("5191", naics))
```

### Generating figures

```{r}
# Creating total annual spending measure
# Removing entries without a 4-digit NAICS code
ann_spending = alldat %>% 
  filter(naics_d4 != "NA") %>%
  
  # Aggregating over fiscal years
  group_by(fiscal_year) %>% 
  summarise(total_prag_obligation = sum(generated_pragmatic_obligation))

# Filtering data to desired NAICS codes
alldat %>% 
  filter(naics_d4 %in% c(5415, 5616, 5191) &
           fiscal_year < 2019 & 
           fiscal_year > 2004) %>%
  
  # Aggregating over NAICS codes, fiscal years
  group_by(fiscal_year, naics_d4) %>%
  summarise(total_GPO = sum(generated_pragmatic_obligation) ) %>%
  
  # Generating plot
  ggplot(aes(x = fiscal_year,
             y = total_GPO / 1e9 ,
             color = naics_d4)) +
  theme_bw() +
  geom_line(size = 1.25) +
  
  # Labels and aesthetic changes
  labs(color = "Spending Category") +
  scale_color_discrete(breaks = c("5415", "5616", "5191"),
                       labels = c("Computer Systems", "Investigation and Security Services", "Information Services")) +
  xlab("Fiscal Year") +
  ylab("Total Spending Obligation (in Billions)") +
  ggtitle("Change in Spending Category Priority") +
  
  # Linear fit line
  stat_smooth(linetype = 2, se = FALSE, method = "lm") +
  ylim(0, 1.5)

# Filtering data to desired NAICS codes
alldat %>% 
  filter(naics_d4 %in% c(5415, 5616, 5191) &
           fiscal_year < 2019 & 
           fiscal_year > 2004) %>%  
  
  # Aggregating over NAICS codes, fiscal years
  group_by(fiscal_year, naics_d4) %>%
  summarise(total_GPO = sum(generated_pragmatic_obligation)) %>%
  left_join(ann_spending, by = c("fiscal_year" = "fiscal_year")) %>%
  
  # Generating plot (scaled to total spending)
  ggplot(aes(x = fiscal_year,
             y = total_GPO / total_prag_obligation ,
             color = naics_d4)) +
  theme_bw() +
  geom_line(size = 1.25) +
  
  # Labels and aesthetic changes
  labs(color = "Spending Category") +
  scale_color_discrete(breaks = c("5415", "5616", "5191"),
                       labels = c("Computer Systems", "Investigation and Security Services", "Information Services")) +
  xlab("Fiscal Year") +
  ylab("Proportional Spending Obligation") +
  ggtitle("Change in Spending Category Priority (Proportional)") +

  # Linear fit line
  stat_smooth(linetype = 2, se = FALSE, method = "lm") +
  ylim(0,.3)
```

### Relationships between immigration rate and spending

```{r}
joined_app = alldat %>% 
  select(generated_pragmatic_obligation, action_date) %>% 
  mutate(month = month(action_date), 
         year  = year(action_date)) %>%
  group_by(year) %>%
  
  # Calculating sum of generated pragmatic obligation
  summarise(total_spending = sum(generated_pragmatic_obligation) / 1e9,
            month = 1) %>%
  
  # Creating new date column for plotting
  mutate(plotdate = as.Date(paste("1-", month,"-", year, sep = ""), format = "%d-%m-%Y")) %>%
  
  # Full joining to get matches between all years  
  full_join(app_cleaned, copy = TRUE) %>%
  
  # Rescaling frequency for easier visualization
  mutate(frequency = frequency / 1e6)
  
# Creating conversion factor (difference in scale) for two-axis plot
conversion_factor = max(na.omit(joined_app$frequency)) / max(na.omit(joined_app$total_spending))

# Creating plot
joined_app %>% 
  
  # Focusing on total immigration + Mexico
  filter(origin == "Total" | origin == "Mexico") %>%
  filter(year > 2004) %>%
  
  # Initializing plot
  ggplot(aes(x = plotdate,
             color = origin)) +
  
  # Adding apprehension line
  geom_line(aes(y = frequency), size = 1.25) +
  
  # Adding spending line (scaled)
  geom_line(aes(y = total_spending * conversion_factor, color = "Spending"), size = 1.25) +
  
  # Manual color changes, aesthetic fixes
  scale_color_manual(breaks = c("Total", "Mexico", "Spending"),
                     values = c("blue", "red", "black"),
                     name = "Value") + 
  xlab("Year") +
  scale_y_continuous(sec.axis = sec_axis(~./conversion_factor,
                                         name = "Total Pragmatic Obligation (in billions)")) +

  ylab("Frequency of Apprehensions (in millions)") +
  ggtitle("Apprehension Frequency vs. Total Spending")

# Correlations between these two:
cor(na.omit(joined_app$frequency[joined_app$origin == "Mexico"]), 
    na.omit(joined_app$total_spending[joined_app$origin == "Mexico"]))
  
cor(na.omit(joined_app$frequency[joined_app$origin == "Total"]), 
    na.omit(joined_app$total_spending[joined_app$origin == "Total"]))

# Generally, these seem weakly correlated
```

